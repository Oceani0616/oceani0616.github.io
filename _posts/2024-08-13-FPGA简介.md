---
layout:     post
title:      "FPGA简介"
subtitle:   "各种计算芯片都说一说"
date:       2024-08-13 10:09:29
author:     "Oceani Cheng"
Comments:    true
header-img: "img/cuhkeat_lq_bg.jpg"
tags:
    - Alita
    - FPGA
    - Note 
---
# FPGA 简介
## 1 **ASIC（专用集成电路）** [click](https://m.huxiu.com/article/2510954.html)
**ASIC（Application Specific Integrated Circuit，专用集成电路）**起步于上世纪70-80年代。早期的时候，曾用于计算机。后来，主要用于嵌入式控制。
这几年开始崛起，用于AI推理、高速搜索以及视觉和图像处理等。
为啥这几年崛起？进入21世纪后，算力需求呈现两个显著趋势：
- 一，算力的使用场景，开始细分；
- 二，用户对算力性能的要求，越来越高。通用的算力芯片，已经无法满足用户的需求。

<br>
<br>
于是，越来越多的企业，开始加强对专用计算芯片的研究和投资力度。而ASIC，就是一种专用于特定任务的芯片。<br><br>
ASIC的官方定义是指：应特定用户的要求，或特定电子系统的需要，专门设计、制造的集成电路。<br><br>
说到ASIC，我们就不得不提到Google公司大名鼎鼎的TPU。**TPU，全称Tensor Processing Unit**，张量处理单元。<br><br>
目前，几乎所有的机器学习系统，都使用张量作为基本数据结构。所以，张量处理单元，我们可以简单理解为“AI处理单元”。<br><br>
2015年，为了更好地完成自己的深度学习任务，提升AI算力，Google推出了一款专门用于神经网络训练的芯片，也就是TPU v1。<br><br>
相比传统的CPU和GPU，在神经网络计算方面，TPU v1可以获得15~30倍的性能提升，能效提升更是达到30~80倍，给行业带来了很大震动。<br><br>
2017年和2018年，Google又再接再励，推出了能力更强的TPU v2和TPU v3，用于AI训练和推理。2021年他们推出了TPU v4，采用7nm工艺，晶体管数达到220亿，性能相较上代提升了10倍，比英伟达的A100还强1.7倍。<br><br>
除了Google之外，还有很多大厂这几年也在捣鼓ASIC。<br><br>
英特尔公司在2019年底收购了以色列AI芯片公司Habana Labs，2022年发布了Gaudi 2 ASIC芯片。IBM研究院则于2022年底发布了AI ASIC芯片AIU。<br><br>
三星早几年也搞过ASIC，当时做的是矿机专用芯片。没错，很多人认识ASIC，就是从比特币挖矿开始的。相比GPU和CPU挖矿，ASIC矿机的效率更高，能耗更低。<br><br>
除了TPU和矿机之外，另外两类很有名的ASIC芯片，是**DPU**和**NPU**。<br><br>
DPU是数据处理单元（Data Processing Unit），主要用于数据中心。详见后面章节。<br><br>
NPU叫做神经网络处理单元（Neural Processing Unit），在电路层模拟人类神经元和突触，并用深度学习指令集处理数据。NPU专门用于神经网络推理，能够实现高效的卷积、池化等操作。一些手机芯片里，经常集成这玩意。说到手机芯片，值得一提的是，我们手机现在的主芯片，也就是常说的SoC芯片，其实也是一种ASIC芯片。<br><br>
ASIC芯片是基于芯片所面向的专项任务，芯片的计算能力和计算效率都是严格匹配于任务算法的。芯片的核心数量，逻辑计算单元和控制单元比例，以及缓存等，整个芯片架构，也是精确定制的。<br><br>
所以，定制专用芯片ASIC，可以实现极致的体积、功耗。这类芯片的可靠性、保密性、算力、能效，都会比通用芯片（CPU、GPU）更强。<br><br>
大家会发现，前面我们提到的几家ASIC公司，都是Google、英特尔、IBM、三星这样的大厂。<br><br>
这是因为，对芯片进行定制设计，对一家企业的研发技术水平要求极高，且耗资极为巨大。<br><br>
做一款ASIC芯片，首先要经过代码设计、综合、后端等复杂的设计流程，再经过几个月的生产加工以及封装测试，才能拿到芯片来搭建系统。<br><br>
大家都听说过“**流片（Tape-out）**”。像流水线一样，通过一系列工艺步骤制造芯片，就是流片。简单来说，就是试生产。<br><br>
ASIC的研发过程是需要流片的。14nm工艺，流片一次需要300万美元左右。5nm工艺，更是高达4725万美元。<br><br>
流片一旦失败，钱全部打水漂，还耽误了大量的时间和精力。一般的小公司，根本玩不起。<br><br>
那么，是不是小公司就无法进行芯片定制了呢？<br><br>
当然不是。接下来，就轮到主角神器出场了，那就是FPGA。<br><br>

## 2 FPGA
**现场可编程门阵列（Field-Programmable Gate Array，FPGA）**是一种半导体器件，具有高密度的可编程逻辑块和可配置的连线网络，用户可以根据需求进行编程和配置。
FPGA这些年在行业里很火，势头比ASIC还猛，甚至被人称为“万能芯片”。其实，简单来说，FPGA就是可以重构的芯片。它可以根据用户的需要，在制造后，进行无限次数的重复编程，以实现想要的数字逻辑功能。
之所以FPGA可以实现DIY，是因为其独特的**架构**。
FPGA由**可编程逻辑块（Configurable Logic Blocks，CLB）**、**输入/输出模块（I/O Blocks，IOB**）、**可编程互连资源（Programmable Interconnect Resources，PIR）**等三种可编程电路，以及**静态存储器SRAM**共同组成。
![git](/img/post_ka250/fpga_config.png)
<p class="caption" > FPGA的基本构造：CLB+IOB+PIR+SRAM </p>

- **CLB**是FPGA中最重要的部分，是实现逻辑功能的基本单元，承载主要的电路功能。它们通常规则排列成一个阵列（逻辑单元阵列，LCA，Logic Cell Array），散布于整个芯片中。
   - **CLB**本身，又主要由**查找表（Look-Up Table，LUT）**、**多路复用器（Multiplexer）**和**触发器（Flip-Flop）**构成。它们用于承载电路中的一个个逻辑“门”，可以用来实现复杂的逻辑功能。
   - 简单来说，我们可以把**LUT**理解为存储了计算结果的**RAM**。当用户描述了一个逻辑电路后，软件会计算所有可能的结果，并写入这个RAM。每一个信号进行逻辑运算，就等于输入一个地址，进行查表。LUT会找出地址对应的内容，返回结果。这种**“硬件化”的运算方式**，显然具有更快的运算速度。用户使用FPGA时，可以通过硬件描述语言（Verilog或VHDL），完成的电路设计，然后对FPGA进行“编程”（烧写），将设计加载到FPGA上，实现对应的功能。
   - 加电时，FPGA将EPROM（可擦编程只读存储器）中的数据读入SRAM中，配置完成后，FPGA进入工作状态。掉电后，FPGA恢复成白片，内部逻辑关系消失。如此反复，就实现了“现场”定制。
   - FPGA的功能非常强大。理论上，如果FPGA提供的门电路规模足够大，通过编程，就能够实现任意ASIC的逻辑功能

- **IOB**主要完成芯片上的逻辑与外部引脚的接口，通常排列在芯片的四周。

- **PIR**提供了丰富的连线资源，包括**纵横网状连线**、**可编程开关矩阵**和**可编程连接点**等。它们实现连接的作用，构成特定功能的电路。

- **静态存储器SRAM**，用于存放内部IOB、CLB和PIR的编程数据，并形成对它们的控制，从而完成系统逻辑功能。

**FPGA的发展历程：**
- 1985年，发明者是**Xilinx公司（赛灵思）**。后来，Altera（阿尔特拉）、Lattice（莱迪思）、Microsemi（美高森美）等公司也参与到FPGA这个领域，并最终形成了四巨头的格局。
- FPGA是在PAL（可编程阵列逻辑）、GAL（通用阵列逻辑）等可编程器件的基础上发展起来的产物，属于一种半定制电路。
- 2015年5月，英特尔以167亿美元的天价收购了Altera，后来收编为PSG（可编程解决方案事业部）部门。
   - 2023年10月，英特尔宣布计划拆分PSG部门，业务独立运营。
- 2020年，英特尔的竞争对手AMD也不甘示弱，以**350亿美元**收购了Xilinx。
   - Xilinx is now part of AMD. Each of us at AMD has the freedom to collaborate to create new ideas and possibilities. We break barriers because we ignore hierarchies. We shape the future because we believe ideas can come from anywhere. And we do it for our people, our customers and the world around us, every day. Join us!
- 国内FPGA厂商的包括复旦微电、紫光国微、安路科技、东土科技、高云半导体、京微齐力、京微雅格、智多晶、遨格芯等。看上去数量不少，但实际上技术差距很大。

**FPGA和ASIC的区别：**
- ASIC就是**用模具来做玩具**。事先要进行开模，比较费事。而且，一旦开模之后，就没办法修改了。如果要做新玩具，就必须重新开模。
- FPGA就像用**乐高积木来搭玩具**。上手就能搭，花一点时间，就可以搭好。如果不满意，或者想搭新玩具，可以拆开，重新搭。
- ASIC与FPGA的很多设计工具是相同的。在设计流程上，FPGA没有ASIC那么复杂，去掉了一些制造过程和额外的设计验证步骤，大概只有ASIC流程的50-70%。最头大的**流片过程**，FPGA是不需要的。
- FPGA可以在实验室或现场进行预制和编程，不需要**一次性工程费用（NRE）**。但是，作为“**通用玩具**”，它的成本是ASIC（压模玩具）的10倍。
- 所以， FGPA单片成本高，少量使用优先选择FGPA。ASIC开模成本高，大量使用才会选择ASIC。
![git](/img/post_ka250/fpgavsasic.jpeg)
<p class="caption" > 40万片是个临界点。 </p>
- FPGA是通用可编辑的芯片，**冗余功能**比较多。不管你怎么设计，都会多出来一些部件。ASIC是**贴身定制**，没什么浪费，且采用**硬连线**。所以，性能更强，功耗更低。
- FPGA现在多用于**产品原型的开发**、设计迭代，以及一些低产量的特定应用。它适合那些开发周期必须短的产品。FPGA还经常用于ASIC的验证。
- ASIC用于设计规模大、复杂度高的芯片，或者是成熟度高、产量比较大的产品。
- FPGA的主要应用领域是**通信、国防、航空、数据中心、医疗、汽车及消费电子**。汽车和工业领域，主要是看重了**FPGA的超低时延优势**，所以会用在ADAS（高级驾驶辅助系统）和伺服电机驱动上。
- FPGA在通信领域应用得很早。很多基站的处理芯片（基带处理、波束赋形、天线收发器等），都是用的FPGA。核心网的编码和协议加速等，也用到它。数据中心之前在DPU等部件上也用FPGA。后来，很多技术成熟了、定型了，通信设备商们就开始用ASIC替代，以此减少成本。消费电子用FPGA，是因为产品迭代太快。ASIC的开发周期太长了，等做出东西来，黄花菜都凉了。

**FPGA、ASIC、GPU，谁是最合适的AI芯片？**

1. **性能**：FPGA/ASIC最强
单纯从理论和架构的角度，ASIC和FPGA的性能和成本，肯定是优于CPU和GPU的。
CPU、GPU遵循的是**冯·诺依曼架构**，指令要经过**存储、译码、执行**等步骤，共享内存在使用时，要经历**仲裁和缓存**。
而FPGA和ASIC是**哈佛架构**。以FPGA为例，它本质上是无指令、无需共享内存的体系结构。FPGA的**逻辑单元功能**在编程时已确定，属于用**硬件来实现软件算**法。对于保存状态的需求，FPGA中的寄存器和片上内存（BRAM）属于各自的控制逻辑，不需要仲裁和缓存。从ALU运算单元占比来看，GPU比CPU高，FPGA因为几乎没有控制模块，所有模块都是ALU运算单元，比GPU更高。综合各个角度，FPGA的运算速度会比GPU更快。

1. **功耗**：FPGA/ASIC最强
GPU的功耗，是出了名的高，单片可以达到250W，甚至**450W（RTX4090）**。而FPGA一般只有**30~50W**。
这主要是因为内存读取。GPU的内存接口（GDDR5、HBM、HBM2、NVLink）**带宽极高**，大约是FPGA传统DDR接口的4-5倍（NVLink是20倍）。但就芯片本身来说，读取DRAM所消耗的能量，是SRAM的100倍以上。**GPU频繁读取DRAM的处理，产生了极高的功耗**。
另外，FPGA的**工作主频**（500MHz以下）比CPU、GPU（1~3GHz）低，也会使得自身功耗更低。FPGA的工作主频低，主要是受**布线资源**的限制。有些线要绕远，时钟频率高了，就来不及。

1. **时延**：FPGA/ASIC最强
GPU通常需要将不同的训练样本，划分成固定大小的“Batch（批次）”，为了最大化达到并行性，需要将数个Batch都集齐，再统一进行处理。
FPGA的架构，是**无批次（Batch-less）**的。每处理完成一个数据包，就能马上输出，时延更有优势。

那么，GPU这里那里都不如FPGA和ASIC，为什么还会成为现在AI计算的大热门呢？

很简单，在对算力性能和规模的极致追求下，现在整个行业根本不在乎什么成本和功耗。

在NVidia的长期努力下，GPU的核心数和工作频率一直在提升，芯片面积也越来越大，属于硬刚算力。**功耗靠工艺制程**，靠水冷等被动散热，反而不着火就行。

英伟达在**软件和生态**方面很会布局。CUDA是GPU的一个核心竞争力。基于CUDA，初学者都可以很快上手，进行GPU的开发。他们苦心经营多年，也形成了群众基础。

相比之下，FPGA和ASIC的开发还是太过复杂，不适合普及。

在**接口方面**，虽然GPU的接口比较单一（主要是PCIe），没有FPGA灵活（FPGA的可编程性，使其能轻松对接任何的标准和非标准接口），但对于服务器来说，足够了，插上就能用。

除了FPGA之外，ASIC之所以在AI上干不过GPU，和它的高昂成本、超长开发周期、巨大开发风险有很大关系。现在AI算法变化很快，ASIC这种开发周期，很要命。

综合上述原因，GPU才有了现在的大好局面。

在AI训练上，GPU的算力强劲，可以大幅提升效率。

在AI推理上，输入一般是单个对象（图像），所以要求要低一点，也不需要什么并行，所以GPU的算力优势没那么明显。很多企业，就会开始采用更便宜、更省电的FPGA或ASIC，进行计算。

其它一些算力场景，也是如此。看重算力绝对性能的，首选GPU。算力性能要求不那么高的，可以考虑FPGA或ASIC，能省则省。

例如，不同算力芯片进行混搭，互相利用优势，叫做**异构计算**。另外，还有**IBM带头搞的类脑芯片**，类似于大脑的神经突触，模拟人脑的处理过程，也获得了突破，热度攀升。

## 1.1.3 DPU[click](https://mp.weixin.qq.com/s/qzr1yk9oLD98fjamoTt1jA)

